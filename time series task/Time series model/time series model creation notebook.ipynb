{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407bf45e-0c60-4d8b-b0b0-896ff4875783",
   "metadata": {},
   "source": [
    "# Import the Needed Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46044df-c7e2-435c-a2d1-1dbc3e469215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "from scipy.signal import periodogram\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "#conda install -c conda-forge xgboost  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a1163-bd92-46a5-ad58-206992f0dce0",
   "metadata": {},
   "source": [
    "# Class DataLoader to Loads CSV files from the specified train  folders into individual datasets (DataFrames),and drops the 'anomaly' column if it exists. The datasets are indexed by the number in the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4abce05-4294-44c2-abba-341b1a7e2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader:\n",
    "    @staticmethod\n",
    "    def load_data(train_folder):\n",
    "        \"\"\"\n",
    "        Loads CSV files from the specified train and test folders into individual datasets (DataFrames),\n",
    "        and drops the 'anomaly' column if it exists. The datasets are indexed by the number in the file name.\n",
    "\n",
    "        Args:\n",
    "        - train_folder (str): Path to the folder containing the train CSV files.\n",
    "        Returns:\n",
    "        - train_datasets (dict): A dictionary where keys are the numbers extracted from train file names\n",
    "                                 and values are their corresponding DataFrames.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the list of train and test CSV files\n",
    "        train_files = [f for f in os.listdir(train_folder) if f.endswith('.csv')]\n",
    "        # Helper function to load a CSV, drop 'anomaly' column if it exists, and extract the number from the file name\n",
    "        def load_and_clean(filepath, filename):\n",
    "            df = pd.read_csv(filepath)\n",
    "            if 'anomaly' in df.columns:\n",
    "                df = df.drop(columns=['anomaly'])\n",
    "            # Extract the number from the filename (e.g., train_9.csv -> 9)\n",
    "            match = re.search(r'\\d+', filename)\n",
    "            if match:\n",
    "                file_number = int(match.group())  # Convert extracted number to int\n",
    "            else:\n",
    "                raise ValueError(f\"No number found in file name: {filename}\")\n",
    "            return file_number, df\n",
    "\n",
    "        # Load train datasets\n",
    "        train_datasets = {}\n",
    "        for f in train_files:\n",
    "            file_number, df = load_and_clean(os.path.join(train_folder, f), f)\n",
    "            train_datasets[file_number] = df  # Use the extracted number as the key\n",
    "\n",
    "        return train_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32bec71-49da-47b7-8f2d-044926f3d8f0",
   "metadata": {},
   "source": [
    "# Define the Dateset_Processing class  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3ecdf-16e6-48c5-9845-5b3cb9fa5742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the JSON file path\n",
    "json_file_path = 'train_datasets_summary.json'\n",
    "with open(json_file_path, 'r') as json_file:\n",
    "        train_dataset_summary = json.load(json_file)\n",
    "\n",
    "print(train_dataset_summary[str(505)])\n",
    "class Dataset_Processing():\n",
    "    def __init__(self, df, key_number, value_column_name):\n",
    "        \"\"\"\n",
    "        Initialize the Dataset_Processing class with the given parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the dataset.\n",
    "        key_number (int): Key or index associated with the dataset (used for missing value filling).\n",
    "        value_column_name (str): The column name for the target variable (e.g., 'value').\n",
    "        type (str): Dataset type, either 'train' or 'test'.\n",
    "        \"\"\"\n",
    "        self.df = copy.deepcopy(df)  # Create a copy of the DataFrame to avoid modifying the original data\n",
    "        self.value = value_column_name  # Name of the column containing target values\n",
    "        self.key_number = key_number  # Key or index to identify the dataset\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    def data_processing(self, lags=5, threshold=0.2):\n",
    "        \"\"\"\n",
    "        Process the dataset by extracting time features, filling missing values, \n",
    "        selecting lags, adding lag features, and extracting Fourier features.\n",
    "\n",
    "        Parameters:\n",
    "        lags (int): Number of lags to consider when selecting lag features.\n",
    "        threshold (float): Threshold for PACF values to determine the number of lags.\n",
    "        \"\"\"\n",
    "        self.extract_time_features()  # Extract time-related features\n",
    "        self.fill_missing_values(self.key_number)  # Fill missing values in the dataset\n",
    "        num_lags = train_dataset_summary[str(self.key_number)]['num_lags']\n",
    "        # self.add_leg_feature(num_lags)  # Add lag features based on the number of selected lags\n",
    "        self.fill_missing_values(self.key_number)  # Refill missing values after adding lag features\n",
    "        self.extract_fourier_features()  # Extract Fourier features\n",
    "\n",
    "\n",
    "    def extract_time_features(self):\n",
    "        \"\"\"\n",
    "        Converts the 'timestamp' column to datetime format, extracts date and time features,\n",
    "        and sorts the DataFrame based on the 'timestamp' column.\n",
    "        \"\"\"\n",
    "        self.df['timestamp'] = pd.to_datetime(self.df['timestamp'], errors='coerce')  # Convert timestamp to datetime\n",
    "        self.df = self.df.sort_values(by='timestamp')  # Sort by the 'timestamp' column\n",
    "\n",
    "        # Extract various time-related features\n",
    "        self.df['year'] = self.df['timestamp'].dt.year\n",
    "        self.df['month'] = self.df['timestamp'].dt.month\n",
    "        self.df['day'] = self.df['timestamp'].dt.day\n",
    "        self.df['dayofweek'] = self.df['timestamp'].dt.dayofweek  # 0 = Monday, 6 = Sunday\n",
    "        self.df['hour'] = self.df['timestamp'].dt.hour\n",
    "        self.df['minute'] = self.df['timestamp'].dt.minute\n",
    "        # Drop the original 'timestamp' column\n",
    "        self.df.drop(columns=['timestamp'], inplace=True)\n",
    "\n",
    "\n",
    "    def fill_missing_values(self, key):\n",
    "        \"\"\"\n",
    "        Fill missing values in the 'value' column and lag columns.\n",
    "\n",
    "        Parameters:\n",
    "        key (int): Key or index of the dataset (used to match with the training set when filling test data).\n",
    "        \"\"\"\n",
    "        # Forward fill to fill missing values in the 'value' column\n",
    "        self.df['value'] = self.df['value'].ffill()\n",
    "\n",
    "        # num_lags = train_dataset_summary[str(self.key_number)]['num_lags']\n",
    "        num_lags = 3\n",
    "  \n",
    "        # For test data, fill missing values using the corresponding training data\n",
    "        # self.df[leg_cols] = self.df[leg_cols].fillna(train_dataset_summary[str(self.key_number)]['mean_lags'])\n",
    "        self.df['value'] = self.df['value'].fillna(self.df['value'].rolling(window=num_lags).mean()).fillna(0)\n",
    "\n",
    "\n",
    "    def select_lags_pacf(self, lags, threshold):\n",
    "        \"\"\"\n",
    "        Select the number of significant lags based on the PACF (Partial Autocorrelation Function).\n",
    "\n",
    "        Parameters:\n",
    "        lags (int): Number of lags to consider for PACF.\n",
    "        threshold (float): Threshold for PACF values to select significant lags.\n",
    "\n",
    "        Returns:\n",
    "        int: The number of significant lags.\n",
    "        \"\"\"\n",
    "        # Calculate PACF values for the target variable\n",
    "        pacf_values = pacf(self.df[self.value], nlags=lags)\n",
    "\n",
    "        # List to store significant lags based on PACF values\n",
    "        significant_lags = []\n",
    "\n",
    "        # Loop through PACF values and store the significant lags\n",
    "        for i, val in enumerate(pacf_values):\n",
    "            if val > threshold or (val < 0 and val < (-1 * threshold)):\n",
    "                significant_lags.append(i)\n",
    "            else:\n",
    "                break  # Stop when the PACF value doesn't meet the threshold\n",
    "\n",
    "        return len(significant_lags) - 1  # Return the number of significant lags\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def extract_fourier_features(self):\n",
    "        \"\"\"\n",
    "        Extract Fourier transformation-based features for time-related periodicity.\n",
    "        \"\"\"\n",
    "        # Fourier features for hour, day, and month periodicity using sine and cosine transformations\n",
    "        self.df['fourier_hour_sin'] = np.sin(2 * np.pi * self.df['hour'] / 24)\n",
    "        self.df['fourier_hour_cos'] = np.cos(2 * np.pi * self.df['hour'] / 24)\n",
    "\n",
    "        self.df['fourier_day_sin'] = np.sin(2 * np.pi * self.df['dayofweek'] / 7)\n",
    "        self.df['fourier_day_cos'] = np.cos(2 * np.pi * self.df['dayofweek'] / 7)\n",
    "\n",
    "        self.df['fourier_month_sin'] = np.sin(2 * np.pi * self.df['month'] / 12)\n",
    "        self.df['fourier_month_cos'] = np.cos(2 * np.pi * self.df['month'] / 12)\n",
    "\n",
    "\n",
    "    def extract_trend(self, window_size=2):\n",
    "        \"\"\"\n",
    "        Extract trend features from the time series using a moving average with a specified window size.\n",
    "\n",
    "        Parameters:\n",
    "        window_size (int): The window size for the moving average (default is 2).\n",
    "        \"\"\"\n",
    "        # Calculate the moving average\n",
    "        trend = self.df[self.value].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "        # Add the trend as a new column in the DataFrame\n",
    "        self.df['trend'] = trend\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63522c-4812-47bf-a5a9-232b1153daf7",
   "metadata": {},
   "source": [
    "# Define the TimeSeriesPlotter, which is used for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6a3a82f-e572-4799-9ec1-2836d5f41726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesPlotter:\n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Initializes the TimeSeriesPlotter with the given DataFrame.\n",
    "\n",
    "        Args:\n",
    "        - df (pd.DataFrame): The DataFrame containing the time series data.\n",
    "        \"\"\"\n",
    "        self.df = copy.deepcopy(df)\n",
    "\n",
    "    def plot_pacf_graph(self, value_col, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Plots the Partial Autocorrelation Function (PACF) for the specified value column.\n",
    "\n",
    "        Args:\n",
    "        - value_col (str): The column name for the values.\n",
    "        - alpha (float): The significance level for the confidence interval (default: 0.05).\n",
    "\n",
    "        Returns:\n",
    "        - None: Displays the PACF plot.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plot_pacf(self.df[value_col], lags=10, alpha=alpha)\n",
    "        plt.title('Partial Autocorrelation Function (PACF)')\n",
    "        plt.xlabel('Lags')\n",
    "        plt.ylabel('PACF')\n",
    "        plt.show()\n",
    "        \n",
    "    @staticmethod\n",
    "    def plot_actual_vs_predicted(y_true, y_pred, title=\"plot_actual_vs_predicted\", save_path=None):\n",
    "        \"\"\"Plots and optionally saves the actual vs predicted time series.\n",
    "\n",
    "        Args:\n",
    "            y_true (array-like): Actual values.\n",
    "            y_pred (array-like): Predicted values.\n",
    "            title (str): The title of the plot.\n",
    "            save_path (str, optional): If provided, the plot will be saved at this location. Defaults to None.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(y_true, label='Actual', color='blue')\n",
    "        plt.plot(y_pred, label='Predicted', linestyle='--', color='orange')\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"Plot saved to {save_path}\")\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "        plt.close()\n",
    "\n",
    "    def plot_aggregated_data(self, value_col='value'):\n",
    "        \"\"\"\n",
    "        Extracts features for grouping (day of week, month, year) and plots the aggregated data.\n",
    "\n",
    "        Args:\n",
    "        - value_col (str): The column name for the values to plot (default: 'value').\n",
    "\n",
    "        Returns:\n",
    "        - None: The function displays the plots.\n",
    "        \"\"\"\n",
    "        # Aggregating the data\n",
    "        weekly_data = self.df.groupby('dayofweek')[value_col].sum().reset_index()\n",
    "        monthly_data = self.df.groupby('month')[value_col].sum().reset_index()\n",
    "        yearly_data = self.df.groupby('year')[value_col].sum().reset_index()\n",
    "\n",
    "        # Create a figure with subplots\n",
    "        plt.figure(figsize=(15, 8))\n",
    "\n",
    "        # Plot for day of week\n",
    "        plt.subplot(3, 1, 1)\n",
    "        plt.plot(weekly_data['dayofweek'], weekly_data[value_col],\n",
    "                 label='Weekly Data', color='blue', linestyle='--', marker='o')\n",
    "        plt.xlabel('Day of Week (0=Mon, 6=Sun)')\n",
    "        plt.ylabel(f'Sum of {value_col}')\n",
    "        plt.title('Weekly Aggregated Data')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot for month\n",
    "        plt.subplot(3, 1, 2)\n",
    "        plt.plot(monthly_data['month'], monthly_data[value_col],\n",
    "                 label='Monthly Data', color='green', linestyle='--', marker='o')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel(f'Sum of {value_col}')\n",
    "        plt.title('Monthly Aggregated Data')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.legend()\n",
    "\n",
    "        # Plot for year\n",
    "        plt.subplot(3, 1, 3)\n",
    "        plt.plot(yearly_data['year'], yearly_data[value_col],\n",
    "                 label='Yearly Data', color='orange', linestyle='--', marker='o')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel(f'Sum of {value_col}')\n",
    "        plt.title('Yearly Aggregated Data')\n",
    "        plt.xticks(yearly_data['year'], rotation=45)\n",
    "\n",
    "        # Add legend and adjust layout\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "\n",
    "    def plot_time_series(self, title='Time Series Data Plot', xlabel='Time', ylabel='Value', color='blue', rotation=45):\n",
    "        \"\"\"\n",
    "        Plots a time series from a DataFrame using year, month, day, hour, minute, second.\n",
    "\n",
    "        Args:\n",
    "        - title (str): The title of the plot (default: 'Time Series Data Plot').\n",
    "        - xlabel (str): The label for the x-axis (default: 'Time').\n",
    "        - ylabel (str): The label for the y-axis (default: 'Value').\n",
    "        - color (str): The color of the plot line (default: 'blue').\n",
    "        - rotation (int): The rotation angle for the x-axis labels (default: 45).\n",
    "\n",
    "        Returns:\n",
    "        - None: The function displays the plot.\n",
    "        \"\"\"\n",
    "        # Create a timestamp column from year, month, day, hour, minute, second\n",
    "        self.df['timestamp'] = pd.to_datetime(self.df[['year', 'month', 'day', 'hour', 'minute', 'second']])\n",
    "\n",
    "        # Create a new figure for the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot the data\n",
    "        plt.plot(self.df['timestamp'], self.df['value'], label='Value over Time', color=color)\n",
    "\n",
    "        # Add labels and title\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.title(title)\n",
    "\n",
    "        # Add a legend\n",
    "        plt.legend()\n",
    "\n",
    "        # Rotate the x-axis labels for better readability\n",
    "        plt.xticks(rotation=rotation)\n",
    "\n",
    "        # Adjust the layout for better display\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5729890d-2103-4706-a02f-3cfdc5edd8ec",
   "metadata": {},
   "source": [
    "# function time_series_train_val_split to split not in random way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60980268-43a4-45a1-8e61-436a7bd665c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_train_val_split(dataset, train_size=0.8):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train and validation sets based on time.\n",
    "    \n",
    "    Parameters:\n",
    "    dataset (DataFrame): The time series dataset.\n",
    "    train_size (float): Proportion of the dataset to include in the train split (between 0 and 1).\n",
    "\n",
    "    Returns:\n",
    "    train (DataFrame): Training dataset.\n",
    "    val (DataFrame): Validation dataset.\n",
    "    \"\"\"\n",
    "    # Calculate the index for the split\n",
    "    split_idx = int(len(dataset) * train_size)\n",
    "    \n",
    "    # Split the dataset while preserving temporal order\n",
    "    train = dataset.iloc[:split_idx]\n",
    "    val = dataset.iloc[split_idx:]\n",
    "    \n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953a901-f24d-479c-bab2-03f888e85d35",
   "metadata": {},
   "source": [
    "# Define Evaluator class which is used to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa48da39-85f1-45e2-b983-ef3f669aa8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    @staticmethod\n",
    "    def evaluate_model_on_data(model_filename, X, y,lag=1):\n",
    "        \"\"\"Evaluates the model performance on training and test data with lagged features.\"\"\"\n",
    "        # Load the model\n",
    "        best_model = ML_Model_Loader.load_model(model_filename)\n",
    "        \n",
    "        # Create lagged features for the training data\n",
    "        X_lagged = []\n",
    "        y_lagged = []\n",
    "\n",
    "        for i in range(len(X) - lag):\n",
    "            X_lagged.append(X[i:i + lag])  # Append the lagged features\n",
    "            y_lagged.append(y[i + lag])    # Append the target variable for the next time step\n",
    "\n",
    "        X_lagged = np.array(X_lagged)\n",
    "        y_lagged = np.array(y_lagged)\n",
    "        n_samples, n_lags, n_features = X_lagged.shape\n",
    "        X_lagged = X_lagged.reshape(n_samples, n_lags * n_features)  # Shape (115, 32)\n",
    "\n",
    "        # Fit the model on the lagged training data\n",
    "        best_model.fit(X_lagged, y_lagged)\n",
    "\n",
    "        # Make predictions on the lagged training data\n",
    "        train_predictions = best_model.predict(X_lagged)\n",
    "\n",
    "        # Evaluate the model's performance on the training data\n",
    "        train_mse = mean_squared_error(y_lagged, train_predictions)\n",
    "        train_rmse = np.sqrt(train_mse)\n",
    "        print(f\"Training RMSE: {train_rmse}\")\n",
    "\n",
    "        return train_rmse, train_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b527a-b30a-4f8d-9e67-604ef4320601",
   "metadata": {},
   "source": [
    "# Define class MLModelGetter to get the best model with the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0fe1fc7c-d2f9-405c-9f96-eec4efd699fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModelGetter:\n",
    "    def __init__(self, train_datasets_split, val_datasets_split, models):\n",
    "        \"\"\"\n",
    "        Initializes the MLModelGetter with training and validation datasets, as well as models and their parameter grids.\n",
    "        \n",
    "        Args:\n",
    "            train_datasets_split (dict): Dictionary of training datasets.\n",
    "            val_datasets_split (dict): Dictionary of validation datasets.\n",
    "            models (dict): Dictionary of models and corresponding hyperparameter grids.\n",
    "        \"\"\"\n",
    "        self.train_datasets_split = copy.deepcopy(train_datasets_split)\n",
    "        self.val_datasets_split = copy.deepcopy(val_datasets_split)\n",
    "        self.models = models\n",
    "        self.best_models_info = {}\n",
    "        self.X_train_dict = {}\n",
    "        self.y_train_dict = {}\n",
    "        self.X_val_dict = {}\n",
    "        self.y_val_dict = {}\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepares the features (X) and target values (y) for both training and validation datasets by splitting the data.\n",
    "        \"\"\"\n",
    "        for key, train in self.train_datasets_split.items():\n",
    "            val = self.val_datasets_split[key]\n",
    "            self.y_train_dict[key] = train['value']\n",
    "            self.X_train_dict[key] = train\n",
    "            self.y_val_dict[key] = val['value']\n",
    "            self.X_val_dict[key] = val\n",
    "            \n",
    "\n",
    "\n",
    "    def get_best_models(self):\n",
    "        \"\"\"\n",
    "        Finds and returns the best models for each dataset by performing grid search with cross-validation.\n",
    "        Chooses the best model based on the lowest validation MSE (val_mse).\n",
    "        Saves the best models and their information.\n",
    "    \n",
    "        Returns:\n",
    "            dict: A dictionary containing information about the best models for each dataset.\n",
    "        \"\"\"\n",
    "        self.prepare_data()\n",
    "    \n",
    "        for key in self.train_datasets_split.keys():\n",
    "            X_train = self.X_train_dict[key]\n",
    "            X_train = X_train.reset_index(drop=True)\n",
    "\n",
    "            y_train = self.y_train_dict[key]\n",
    "            y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "            X_val = self.X_val_dict[key]\n",
    "            X_val = X_val.reset_index(drop=True)\n",
    "            \n",
    "            y_val = self.y_val_dict[key]\n",
    "            y_val = y_val.reset_index(drop=True)\n",
    "\n",
    "            lag=train_dataset_summary[str(key)]['num_lags']\n",
    "\n",
    "            best_params = {}\n",
    "            best_scores = {}\n",
    "            best_val_mse = float('inf')  # Initialize with a high value to find the minimum\n",
    "            best_model_info = None  # To store the best model's information\n",
    "\n",
    "            # Loop through each model and perform GridSearchCV\n",
    "            for model_name, (model, param_grid) in self.models.items():\n",
    "                grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "                # Create lagged features for X_train\n",
    "                X_train_legged, y_train_legged = [], []\n",
    "                for i in range(0, len(X_train) - lag):\n",
    "                    if (i + lag) < len(y_val):\n",
    "                        X_train_legged.append(X_train[i:i + lag]) \n",
    "                        y_train_legged.append(y_train[i + lag])\n",
    "            \n",
    "                X_train_legged = np.array(X_train_legged)\n",
    "                y_train_legged = np.array(y_train_legged)\n",
    "                n_samples, n_lags, n_features = X_train_legged.shape\n",
    "                X_train_legged = X_train_legged.reshape(n_samples, n_lags * n_features)  # Shape (115, 32)\n",
    "\n",
    "                y_train_legged = np.array(y_train_legged)  # Shape (115,)            \n",
    "                # Fit GridSearchCV with the lagged training data\n",
    "                grid_search.fit(X_train_legged, y_train_legged)\n",
    "                # Get the best model from GridSearchCV\n",
    "                best_model = grid_search.best_estimator_\n",
    "\n",
    "                # Create lagged features for X_val\n",
    "                X_val_legged, y_val_legged = [], []\n",
    "                for i in range(0, len(X_val) - lag):\n",
    "                    if (i + lag) < len(y_val):\n",
    "                        X_val_legged.append(X_val[i:i + lag])  \n",
    "                        y_val_legged.append(y_val[i + lag])\n",
    "            \n",
    "                X_val_legged = np.array(X_val_legged)\n",
    "                y_val_legged = np.array(y_val_legged)\n",
    "                n_samples, n_lags, n_features = X_val_legged.shape\n",
    "                X_val_legged = X_val_legged.reshape(n_samples, n_lags * n_features)  # Shape (115, 32)\n",
    "                y_val_legged = np.array(y_train_legged)  # Shape (115,)\n",
    "\n",
    "                # Predict on the validation set and calculate MSE\n",
    "                y_val_pred = best_model.predict(X_val_legged)\n",
    "                val_mse = mean_squared_error(y_val_legged, y_val_pred)\n",
    "                y_val_pred = best_model.predict(X_val_legged)\n",
    "                val_mse = mean_squared_error(y_val_legged, y_val_pred)\n",
    "\n",
    "                # Get the best parameters and score (cross-validated training score)\n",
    "                best_params[model_name] = grid_search.best_params_\n",
    "                best_scores[model_name] = -grid_search.best_score_  # Negate to get the positive MSE\n",
    "\n",
    "                # If current model has a lower validation MSE, update the best model info\n",
    "                if val_mse < best_val_mse:\n",
    "                    best_val_mse = val_mse\n",
    "                    best_model_info = {\n",
    "                        'Best Model': model_name,\n",
    "                        'Best Parameters': grid_search.best_params_,\n",
    "                        'Best Validation MSE': val_mse,\n",
    "                        'Model Instance': best_model  # Save the best model instance\n",
    "                        }\n",
    "\n",
    "            # Store the best model info for the current dataset\n",
    "            if best_model_info:\n",
    "                self.best_models_info[key] = best_model_info\n",
    "\n",
    "                # Print the best model details for the current dataset\n",
    "                print(f\"Best Model for dataset {key}: {best_model_info['Best Model']}\")\n",
    "                print(f\"Best Parameters for dataset {key}: {best_model_info['Best Parameters']}\")\n",
    "                print(f\"Best Validation MSE for dataset {key}: {best_model_info['Best Validation MSE']}\\n\")\n",
    "\n",
    "                # Save the best model to a pickle file\n",
    "                self.save_best_model(best_model_info['Best Model'], key, best_model_info['Model Instance'])\n",
    "\n",
    "        return self.best_models_info\n",
    "\n",
    "\n",
    "    def save_best_model(self, model_name, dataset_index, model_instance):\n",
    "        \"\"\"\n",
    "        Saves the best model to a pickle file for later use.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The name of the best model.\n",
    "            dataset_index (str/int): The index or key of the dataset.\n",
    "            model_instance: The instance of the best model to be saved.\n",
    "        \"\"\"\n",
    "        # Create a filename based on the dataset index\n",
    "        filename = f\"{dataset_index}_model.pkl\"\n",
    "        # Save the model to a pickle file\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(model_instance, file)\n",
    "        print(f\"Saved best model for dataset {dataset_index} as {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b24209-1086-4482-8223-b68539da8bbb",
   "metadata": {},
   "source": [
    "# Define ML_Model_Loader which helps in save and load the model in pickle file form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "823d8462-854d-4bd9-89c6-8c4d060eab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_Model_Loader:\n",
    "    @staticmethod\n",
    "    def save_model(model, filename):\n",
    "        \"\"\"Saves a model to a pickle file.\n",
    "\n",
    "        Args:\n",
    "            model: The model object to save.\n",
    "            filename (str): The name of the file to save the model to.\n",
    "        \"\"\"\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(filename):\n",
    "        \"\"\"Loads a model from a pickle file.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The name of the file to load the model from.\n",
    "\n",
    "        Returns:\n",
    "            The loaded model.\n",
    "        \"\"\"\n",
    "        with open(filename, 'rb') as file:\n",
    "            model = pickle.load(file)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0e11c-2896-455e-b849-f7280dd044f0",
   "metadata": {},
   "source": [
    "# Read the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91b6782c-ee2a-4b16-9b3f-7ebc448cd40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"Train\"\n",
    "train_datasets = DataLoader.load_data(train_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7729275-08d0-470c-89e0-5fb2d7cc91c1",
   "metadata": {},
   "source": [
    "# Doing preprossing for both train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92323d8f-729f-48a2-87c2-bb37f75b6127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>fourier_hour_sin</th>\n",
       "      <th>fourier_hour_cos</th>\n",
       "      <th>fourier_day_sin</th>\n",
       "      <th>fourier_day_cos</th>\n",
       "      <th>fourier_month_sin</th>\n",
       "      <th>fourier_month_cos</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.450116</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.450116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.393450</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.028333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.853038</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.623244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.865014</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.859026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014318</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.425348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      value  year  month  day  dayofweek  hour  minute  fourier_hour_sin  \\\n",
       "0  0.450116  2021      7    1          3     0       0               0.0   \n",
       "1 -0.393450  2021      7    2          4     0       0               0.0   \n",
       "2 -0.853038  2021      7    3          5     0       0               0.0   \n",
       "3 -0.865014  2021      7    4          6     0       0               0.0   \n",
       "4  0.014318  2021      7    5          0     0       0               0.0   \n",
       "\n",
       "   fourier_hour_cos  fourier_day_sin  fourier_day_cos  fourier_month_sin  \\\n",
       "0               1.0         0.433884        -0.900969               -0.5   \n",
       "1               1.0        -0.433884        -0.900969               -0.5   \n",
       "2               1.0        -0.974928        -0.222521               -0.5   \n",
       "3               1.0        -0.781831         0.623490               -0.5   \n",
       "4               1.0         0.000000         1.000000               -0.5   \n",
       "\n",
       "   fourier_month_cos     trend  \n",
       "0          -0.866025  0.450116  \n",
       "1          -0.866025  0.028333  \n",
       "2          -0.866025 -0.623244  \n",
       "3          -0.866025 -0.859026  \n",
       "4          -0.866025 -0.425348  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in train_datasets:\n",
    "    data_processor = Dataset_Processing(train_datasets[key], key, 'value')\n",
    "    data_processor.data_processing()\n",
    "    data_processor.extract_trend()\n",
    "    # Get the DataFrame from the data processor\n",
    "    train_datasets[key] = data_processor.df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_datasets[102].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927c21e9-4392-4c06-815f-d23553f351bf",
   "metadata": {},
   "source": [
    "# Split the  dataset into training and validation parts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f538b295-19d9-40c8-a4f6-a147fec25132",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets_split = {}\n",
    "val_datasets_split = {}\n",
    "X_train_dict = {}\n",
    "y_train_dict = {}\n",
    "X_val_dict = {}\n",
    "y_val_dict = {}\n",
    "\n",
    "\n",
    "# Splitting the datasets\n",
    "for key, dataset in train_datasets.items():\n",
    "    train, val = time_series_train_val_split(dataset, train_size=0.8)\n",
    "    train_datasets_split[key] = train\n",
    "    val_datasets_split[key] = val\n",
    "\n",
    "    # Prepare training data\n",
    "    y_train_dict[key] = train['value']\n",
    "    X_train_dict[key] = train\n",
    "\n",
    "    # Prepare validation data\n",
    "    y_val_dict[key] = val['value']\n",
    "    X_val_dict[key] = val\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb525a2d-0960-46b4-ad50-42ee34f28dbf",
   "metadata": {},
   "source": [
    "# Get the best model for every dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42d6de52-6073-4084-893d-03328b316902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RMSE: 0.0029834176568009525\n",
      "Dataset 102 - Training RMSE: 0.0029834176568009525\n",
      "Plot saved to plot_Training_102.png\n",
      "Training RMSE: 0.1025776667299522\n",
      "Dataset 103 - Training RMSE: 0.1025776667299522\n",
      "Plot saved to plot_Training_103.png\n",
      "Training RMSE: 0.10720128616086828\n",
      "Dataset 105 - Training RMSE: 0.10720128616086828\n",
      "Plot saved to plot_Training_105.png\n",
      "Training RMSE: 0.12068056841439087\n",
      "Dataset 110 - Training RMSE: 0.12068056841439087\n",
      "Plot saved to plot_Training_110.png\n",
      "Training RMSE: 0.08550447588049973\n",
      "Dataset 115 - Training RMSE: 0.08550447588049973\n",
      "Plot saved to plot_Training_115.png\n",
      "Training RMSE: 0.19615331278106804\n",
      "Dataset 118 - Training RMSE: 0.19615331278106804\n",
      "Plot saved to plot_Training_118.png\n",
      "Training RMSE: 0.1622961268123497\n",
      "Dataset 119 - Training RMSE: 0.1622961268123497\n",
      "Plot saved to plot_Training_119.png\n",
      "Training RMSE: 0.20686029890211788\n",
      "Dataset 12 - Training RMSE: 0.20686029890211788\n",
      "Plot saved to plot_Training_12.png\n",
      "Training RMSE: 0.21521638666740234\n",
      "Dataset 131 - Training RMSE: 0.21521638666740234\n",
      "Plot saved to plot_Training_131.png\n",
      "Training RMSE: 0.10780087593616972\n",
      "Dataset 141 - Training RMSE: 0.10780087593616972\n",
      "Plot saved to plot_Training_141.png\n",
      "Training RMSE: 0.17508364310290606\n",
      "Dataset 151 - Training RMSE: 0.17508364310290606\n",
      "Plot saved to plot_Training_151.png\n",
      "Training RMSE: 0.10707327761958613\n",
      "Dataset 153 - Training RMSE: 0.10707327761958613\n",
      "Plot saved to plot_Training_153.png\n",
      "Training RMSE: 0.1673451841896293\n",
      "Dataset 155 - Training RMSE: 0.1673451841896293\n",
      "Plot saved to plot_Training_155.png\n",
      "Training RMSE: 0.21992005167534148\n",
      "Dataset 161 - Training RMSE: 0.21992005167534148\n",
      "Plot saved to plot_Training_161.png\n",
      "Training RMSE: 0.1417077583011342\n",
      "Dataset 165 - Training RMSE: 0.1417077583011342\n",
      "Plot saved to plot_Training_165.png\n",
      "Training RMSE: 0.12121058696361828\n",
      "Dataset 176 - Training RMSE: 0.12121058696361828\n",
      "Plot saved to plot_Training_176.png\n",
      "Training RMSE: 0.18449912075585223\n",
      "Dataset 177 - Training RMSE: 0.18449912075585223\n",
      "Plot saved to plot_Training_177.png\n",
      "Training RMSE: 0.09531632351118835\n",
      "Dataset 185 - Training RMSE: 0.09531632351118835\n",
      "Plot saved to plot_Training_185.png\n",
      "Training RMSE: 0.05028050978467347\n",
      "Dataset 188 - Training RMSE: 0.05028050978467347\n",
      "Plot saved to plot_Training_188.png\n",
      "Training RMSE: 0.14616004118716308\n",
      "Dataset 198 - Training RMSE: 0.14616004118716308\n",
      "Plot saved to plot_Training_198.png\n",
      "Training RMSE: 0.2648124513937497\n",
      "Dataset 200 - Training RMSE: 0.2648124513937497\n",
      "Plot saved to plot_Training_200.png\n",
      "Training RMSE: 0.022188635197131927\n",
      "Dataset 204 - Training RMSE: 0.022188635197131927\n",
      "Plot saved to plot_Training_204.png\n",
      "Training RMSE: 0.19894418309271833\n",
      "Dataset 205 - Training RMSE: 0.19894418309271833\n",
      "Plot saved to plot_Training_205.png\n",
      "Training RMSE: 0.21742935158368448\n",
      "Dataset 209 - Training RMSE: 0.21742935158368448\n",
      "Plot saved to plot_Training_209.png\n",
      "Training RMSE: 0.08388770703616201\n",
      "Dataset 21 - Training RMSE: 0.08388770703616201\n",
      "Plot saved to plot_Training_21.png\n",
      "Training RMSE: 0.2932514776266079\n",
      "Dataset 211 - Training RMSE: 0.2932514776266079\n",
      "Plot saved to plot_Training_211.png\n",
      "Training RMSE: 0.1948147658182405\n",
      "Dataset 212 - Training RMSE: 0.1948147658182405\n",
      "Plot saved to plot_Training_212.png\n",
      "Training RMSE: 0.260084827240516\n",
      "Dataset 217 - Training RMSE: 0.260084827240516\n",
      "Plot saved to plot_Training_217.png\n",
      "Training RMSE: 0.2709326805726973\n",
      "Dataset 218 - Training RMSE: 0.2709326805726973\n",
      "Plot saved to plot_Training_218.png\n",
      "Training RMSE: 0.20472494688107779\n",
      "Dataset 227 - Training RMSE: 0.20472494688107779\n",
      "Plot saved to plot_Training_227.png\n",
      "Training RMSE: 0.14630205833334553\n",
      "Dataset 23 - Training RMSE: 0.14630205833334553\n",
      "Plot saved to plot_Training_23.png\n",
      "Training RMSE: 0.2665073599836468\n",
      "Dataset 242 - Training RMSE: 0.2665073599836468\n",
      "Plot saved to plot_Training_242.png\n",
      "Training RMSE: 0.14673968538569576\n",
      "Dataset 246 - Training RMSE: 0.14673968538569576\n",
      "Plot saved to plot_Training_246.png\n",
      "Training RMSE: 0.09970754484235848\n",
      "Dataset 251 - Training RMSE: 0.09970754484235848\n",
      "Plot saved to plot_Training_251.png\n",
      "Training RMSE: 0.20746824084670762\n",
      "Dataset 256 - Training RMSE: 0.20746824084670762\n",
      "Plot saved to plot_Training_256.png\n",
      "Training RMSE: 0.16561856743372422\n",
      "Dataset 272 - Training RMSE: 0.16561856743372422\n",
      "Plot saved to plot_Training_272.png\n",
      "Training RMSE: 0.08076927633270316\n",
      "Dataset 287 - Training RMSE: 0.08076927633270316\n",
      "Plot saved to plot_Training_287.png\n",
      "Training RMSE: 0.17231009967241667\n",
      "Dataset 29 - Training RMSE: 0.17231009967241667\n",
      "Plot saved to plot_Training_29.png\n",
      "Training RMSE: 0.2415889009637509\n",
      "Dataset 290 - Training RMSE: 0.2415889009637509\n",
      "Plot saved to plot_Training_290.png\n",
      "Training RMSE: 0.017177719386360195\n",
      "Dataset 294 - Training RMSE: 0.017177719386360195\n",
      "Plot saved to plot_Training_294.png\n",
      "Training RMSE: 0.0007623291035776615\n",
      "Dataset 297 - Training RMSE: 0.0007623291035776615\n",
      "Plot saved to plot_Training_297.png\n",
      "Training RMSE: 0.23930711991991785\n",
      "Dataset 300 - Training RMSE: 0.23930711991991785\n",
      "Plot saved to plot_Training_300.png\n",
      "Training RMSE: 0.17886306002690627\n",
      "Dataset 301 - Training RMSE: 0.17886306002690627\n",
      "Plot saved to plot_Training_301.png\n",
      "Training RMSE: 0.16037840134296663\n",
      "Dataset 306 - Training RMSE: 0.16037840134296663\n",
      "Plot saved to plot_Training_306.png\n",
      "Training RMSE: 0.00768651131698205\n",
      "Dataset 310 - Training RMSE: 0.00768651131698205\n",
      "Plot saved to plot_Training_310.png\n",
      "Training RMSE: 0.16053800506698018\n",
      "Dataset 314 - Training RMSE: 0.16053800506698018\n",
      "Plot saved to plot_Training_314.png\n",
      "Training RMSE: 0.22055537055451288\n",
      "Dataset 321 - Training RMSE: 0.22055537055451288\n",
      "Plot saved to plot_Training_321.png\n",
      "Training RMSE: 0.25281376651587817\n",
      "Dataset 325 - Training RMSE: 0.25281376651587817\n",
      "Plot saved to plot_Training_325.png\n",
      "Training RMSE: 0.13658897088015912\n",
      "Dataset 335 - Training RMSE: 0.13658897088015912\n",
      "Plot saved to plot_Training_335.png\n",
      "Training RMSE: 0.16911941695877503\n",
      "Dataset 338 - Training RMSE: 0.16911941695877503\n",
      "Plot saved to plot_Training_338.png\n",
      "Training RMSE: 0.13344932411750504\n",
      "Dataset 348 - Training RMSE: 0.13344932411750504\n",
      "Plot saved to plot_Training_348.png\n",
      "Training RMSE: 0.17579098916825858\n",
      "Dataset 352 - Training RMSE: 0.17579098916825858\n",
      "Plot saved to plot_Training_352.png\n",
      "Training RMSE: 0.15723620080176887\n",
      "Dataset 354 - Training RMSE: 0.15723620080176887\n",
      "Plot saved to plot_Training_354.png\n",
      "Training RMSE: 0.27861663743003806\n",
      "Dataset 361 - Training RMSE: 0.27861663743003806\n",
      "Plot saved to plot_Training_361.png\n",
      "Training RMSE: 0.17439567228407907\n",
      "Dataset 367 - Training RMSE: 0.17439567228407907\n",
      "Plot saved to plot_Training_367.png\n",
      "Training RMSE: 0.1829531327082695\n",
      "Dataset 370 - Training RMSE: 0.1829531327082695\n",
      "Plot saved to plot_Training_370.png\n",
      "Training RMSE: 0.11003000149731867\n",
      "Dataset 383 - Training RMSE: 0.11003000149731867\n",
      "Plot saved to plot_Training_383.png\n",
      "Training RMSE: 0.23873024010521732\n",
      "Dataset 386 - Training RMSE: 0.23873024010521732\n",
      "Plot saved to plot_Training_386.png\n",
      "Training RMSE: 0.2436259243670007\n",
      "Dataset 39 - Training RMSE: 0.2436259243670007\n",
      "Plot saved to plot_Training_39.png\n",
      "Training RMSE: 0.16955036127365816\n",
      "Dataset 391 - Training RMSE: 0.16955036127365816\n",
      "Plot saved to plot_Training_391.png\n",
      "Training RMSE: 0.11691904335624645\n",
      "Dataset 397 - Training RMSE: 0.11691904335624645\n",
      "Plot saved to plot_Training_397.png\n",
      "Training RMSE: 0.17859543554429766\n",
      "Dataset 401 - Training RMSE: 0.17859543554429766\n",
      "Plot saved to plot_Training_401.png\n",
      "Training RMSE: 0.05231079905575886\n",
      "Dataset 42 - Training RMSE: 0.05231079905575886\n",
      "Plot saved to plot_Training_42.png\n",
      "Training RMSE: 0.12759014341171646\n",
      "Dataset 421 - Training RMSE: 0.12759014341171646\n",
      "Plot saved to plot_Training_421.png\n",
      "Training RMSE: 0.1997817251459959\n",
      "Dataset 428 - Training RMSE: 0.1997817251459959\n",
      "Plot saved to plot_Training_428.png\n",
      "Training RMSE: 0.09515418974030825\n",
      "Dataset 429 - Training RMSE: 0.09515418974030825\n",
      "Plot saved to plot_Training_429.png\n",
      "Training RMSE: 0.10227387835952073\n",
      "Dataset 43 - Training RMSE: 0.10227387835952073\n",
      "Plot saved to plot_Training_43.png\n",
      "Training RMSE: 0.0912308419476748\n",
      "Dataset 430 - Training RMSE: 0.0912308419476748\n",
      "Plot saved to plot_Training_430.png\n",
      "Training RMSE: 0.1424672195451532\n",
      "Dataset 435 - Training RMSE: 0.1424672195451532\n",
      "Plot saved to plot_Training_435.png\n",
      "Training RMSE: 0.18512663652340738\n",
      "Dataset 437 - Training RMSE: 0.18512663652340738\n",
      "Plot saved to plot_Training_437.png\n",
      "Training RMSE: 0.0694410045611092\n",
      "Dataset 439 - Training RMSE: 0.0694410045611092\n",
      "Plot saved to plot_Training_439.png\n",
      "Training RMSE: 0.05844477653172683\n",
      "Dataset 440 - Training RMSE: 0.05844477653172683\n",
      "Plot saved to plot_Training_440.png\n",
      "Training RMSE: 0.10919777343484355\n",
      "Dataset 446 - Training RMSE: 0.10919777343484355\n",
      "Plot saved to plot_Training_446.png\n",
      "Training RMSE: 0.17636828746364444\n",
      "Dataset 447 - Training RMSE: 0.17636828746364444\n",
      "Plot saved to plot_Training_447.png\n",
      "Training RMSE: 0.1990213055693149\n",
      "Dataset 451 - Training RMSE: 0.1990213055693149\n",
      "Plot saved to plot_Training_451.png\n",
      "Training RMSE: 0.21380480575111246\n",
      "Dataset 453 - Training RMSE: 0.21380480575111246\n",
      "Plot saved to plot_Training_453.png\n",
      "Training RMSE: 0.11577455657969708\n",
      "Dataset 461 - Training RMSE: 0.11577455657969708\n",
      "Plot saved to plot_Training_461.png\n",
      "Training RMSE: 0.28278564229698105\n",
      "Dataset 462 - Training RMSE: 0.28278564229698105\n",
      "Plot saved to plot_Training_462.png\n",
      "Training RMSE: 0.17220473450317783\n",
      "Dataset 465 - Training RMSE: 0.17220473450317783\n",
      "Plot saved to plot_Training_465.png\n",
      "Training RMSE: 0.0764360859213559\n",
      "Dataset 469 - Training RMSE: 0.0764360859213559\n",
      "Plot saved to plot_Training_469.png\n",
      "Training RMSE: 0.06555970094716068\n",
      "Dataset 482 - Training RMSE: 0.06555970094716068\n",
      "Plot saved to plot_Training_482.png\n",
      "Training RMSE: 0.22699930534943621\n",
      "Dataset 483 - Training RMSE: 0.22699930534943621\n",
      "Plot saved to plot_Training_483.png\n",
      "Training RMSE: 0.10202966946653654\n",
      "Dataset 490 - Training RMSE: 0.10202966946653654\n",
      "Plot saved to plot_Training_490.png\n",
      "Training RMSE: 0.17158307334613998\n",
      "Dataset 491 - Training RMSE: 0.17158307334613998\n",
      "Plot saved to plot_Training_491.png\n",
      "Training RMSE: 0.2366228970387981\n",
      "Dataset 495 - Training RMSE: 0.2366228970387981\n",
      "Plot saved to plot_Training_495.png\n",
      "Training RMSE: 0.2062238421948113\n",
      "Dataset 498 - Training RMSE: 0.2062238421948113\n",
      "Plot saved to plot_Training_498.png\n",
      "Training RMSE: 0.1785013336196785\n",
      "Dataset 499 - Training RMSE: 0.1785013336196785\n",
      "Plot saved to plot_Training_499.png\n",
      "Training RMSE: 0.2579464665815672\n",
      "Dataset 505 - Training RMSE: 0.2579464665815672\n",
      "Plot saved to plot_Training_505.png\n",
      "Training RMSE: 0.027240278111978627\n",
      "Dataset 506 - Training RMSE: 0.027240278111978627\n",
      "Plot saved to plot_Training_506.png\n",
      "Training RMSE: 0.08148674404327884\n",
      "Dataset 507 - Training RMSE: 0.08148674404327884\n",
      "Plot saved to plot_Training_507.png\n",
      "Training RMSE: 0.0006159086639163729\n",
      "Dataset 66 - Training RMSE: 0.0006159086639163729\n",
      "Plot saved to plot_Training_66.png\n",
      "Training RMSE: 0.08803203083668223\n",
      "Dataset 74 - Training RMSE: 0.08803203083668223\n",
      "Plot saved to plot_Training_74.png\n",
      "Training RMSE: 0.14327780114842092\n",
      "Dataset 75 - Training RMSE: 0.14327780114842092\n",
      "Plot saved to plot_Training_75.png\n",
      "Training RMSE: 0.31175587262286797\n",
      "Dataset 79 - Training RMSE: 0.31175587262286797\n",
      "Plot saved to plot_Training_79.png\n",
      "Training RMSE: 0.24238734840126522\n",
      "Dataset 9 - Training RMSE: 0.24238734840126522\n",
      "Plot saved to plot_Training_9.png\n",
      "Training RMSE: 0.11351731166067354\n",
      "Dataset 92 - Training RMSE: 0.11351731166067354\n",
      "Plot saved to plot_Training_92.png\n"
     ]
    }
   ],
   "source": [
    "# Example usage for evaluating all datasets and saving plots using TimeSeriesPlotter\n",
    "for dataset_index in train_datasets_split.keys():\n",
    "    # Define file names based on the dataset index\n",
    "    model_filename = f\"{dataset_index}_model.pkl\"  # Use the index for each saved model file\n",
    "    \n",
    "    # Get the corresponding training and test datasets\n",
    "    X_train = X_train_dict[dataset_index]\n",
    "    y_train = y_train_dict[dataset_index]\n",
    "    # Evaluate the model using the Evaluator class\n",
    "    train_rmse, train_predictions=  Evaluator.evaluate_model_on_data(model_filename, X_train, y_train,train_dataset_summary[str(dataset_index)]['num_lags'])\n",
    "\n",
    "    # Print the RMSE results for each dataset\n",
    "    print(f\"Dataset {dataset_index} - Training RMSE: {train_rmse}\")\n",
    "\n",
    "    # Define file paths for saving the plots\n",
    "    training_plot_path = f'plot_Training_{dataset_index}.png'\n",
    "\n",
    "    # Use TimeSeriesPlotter to plot and save the results for training data\n",
    "    TimeSeriesPlotter.plot_actual_vs_predicted(y_train, train_predictions, f'Training: Actual vs Predicted for Dataset {dataset_index}', training_plot_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8a2a8d-b8ca-4824-a58d-61af179f6639",
   "metadata": {},
   "source": [
    "# Hybrid Model\r\n",
    "## Trying using Hybrid Model one to extract the trend and another to extract the seasonality\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "aaa1d7ee-00d2-4ffd-93d6-095979c5c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You'll add fit and predict methods to this minimal class\n",
    "class BoostedHybrid:\n",
    "    def __init__(self, model_1, model_2):\n",
    "        \"\"\"\n",
    "        Class BoostedHybrid :\n",
    "        Parameters: \n",
    "        model_1: model is used to extract the trend\n",
    "        model_2: model is used to extract the seasonlity\n",
    "        \"\"\"\n",
    "        self.model_1 = model_1\n",
    "        self.model_2 = model_2\n",
    "        self.poly = False\n",
    "        self.poly_model = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        mothed predict: use to make predictions\n",
    "        parameters:\n",
    "        inputs:\n",
    "        X: the features the Features for the both models\n",
    "        outputs:\n",
    "        y_pred: the model prediction \n",
    "        \"\"\"\n",
    "        t = copy.deepcopy(X)\n",
    "        dp = DeterministicProcess(index=X.index, order=1)\n",
    "        # X_1: Features for model_1 (Trend)\n",
    "        X_1 = dp.in_sample()\n",
    "        # X_2: Features for model_2 (Seasonality)\n",
    "        X_2 = copy.deepcopy(X)\n",
    "        if self.poly: # doing transform if the given model is poly transform only\n",
    "            X_1 = self.poly_model.transform(X_1)\n",
    "        # Predict trend and add seasonal predictions\n",
    "        y_pred = self.model_1.predict(X_1)\n",
    "        y_pred += self.model_2.predict(X_2)\n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Method fit: fit the given data to train the model \n",
    "        parameters:\n",
    "        inputs:\n",
    "        X: the features for the both model_1 and model_2\n",
    "        y: the output of the train data\n",
    "        \n",
    "        \"\"\"\n",
    "        t = copy.deepcopy(X)\n",
    "        # X_1: Features for model_1 (Trend)\n",
    "        dp = DeterministicProcess(index=X.index, order=1)\n",
    "        X_1 = dp.in_sample()\n",
    "        # X_2: Features for model_2 (Seasonality)\n",
    "        X_2 = copy.deepcopy(X)\n",
    "\n",
    "        # Check if model_1 is a PolynomialFeatures instance\n",
    "        if isinstance(self.model_1, PolynomialFeatures):\n",
    "            self.poly = True\n",
    "            X_1 = self.model_1.fit_transform(X_1)\n",
    "            self.poly_model = copy.deepcopy(self.model_1)\n",
    "            self.model_1 = LinearRegression()\n",
    "        # Fit the trend model\n",
    "        self.model_1.fit(X_1, y)\n",
    "\n",
    "        # Get fitted values and calculate residuals\n",
    "        y_fit = self.model_1.predict(X_1)\n",
    "        y_resid = y - y_fit\n",
    "\n",
    "        # Fit the seasonal model on residuals\n",
    "        self.model_2.fit(X_2, y_resid)\n",
    "\n",
    "    def save_model(self, file_path):\n",
    "        \"\"\"\n",
    "        method: save_model: used in saving the model in a given path\n",
    "        parameters: \n",
    "        file_path: the path that want to save the model in.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'wb') as f:\n",
    "            # Save models and their state\n",
    "            pickle.dump({\n",
    "                'model_1': self.model_1,\n",
    "                'model_2': self.model_2,\n",
    "                'poly': self.poly,\n",
    "                'poly_model': self.poly_model\n",
    "            }, f)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"\n",
    "        method: \n",
    "        get_params: used to get the parameters of the model \n",
    "        outputs:\n",
    "        return the hybird model parameters.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'model_1': self.model_1,\n",
    "            'model_2': self.model_2\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"\n",
    "        method: \n",
    "        set_params: used to set the parameters of the model \n",
    "        inputs:\n",
    "         **params:the hybird model parameters that want to set.\n",
    "        \"\"\"\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15344a48-7480-49bc-aabd-6bd168330c1b",
   "metadata": {},
   "source": [
    "#  Define Model_Getter to get the best hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a2fb7-5681-4313-aee8-d4616c546d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_Getter:\n",
    "    def __init__(self, train_dataset, y_column='value'):\n",
    "        \"\"\"\n",
    "        Initializes the Model_Getter class with the training dataset and target column.\n",
    "\n",
    "        Parameters:\n",
    "        train_dataset (DataFrame): The dataset used for training the models.\n",
    "        y_column (str): The name of the target variable column. Default is 'value'.\n",
    "        \"\"\"\n",
    "        self.train_dataset = copy.deepcopy(train_dataset)\n",
    "        self.y = self.train_dataset[y_column]\n",
    "        self.X = self.train_dataset.drop('value', axis=1)\n",
    "        self.best_model = None\n",
    "        self.best_score = float('inf')\n",
    "\n",
    "    def define_models(self):\n",
    "        \"\"\"Defines trend and seasonality models.\"\"\"\n",
    "        # Define trend models\n",
    "        self.trend_models = [\n",
    "            LinearRegression(),  # Simple linear regression\n",
    "            Ridge(alpha=1.0),\n",
    "            PolynomialFeatures(degree=2), \n",
    "        ]\n",
    "\n",
    "        # Define seasonality models with their parameter grids\n",
    "        self.seasonality_models = {\n",
    "            'XGBRegressor': (XGBRegressor(),{'max_depth':[3,5] ,'n_estimators': [150, 200]}),\n",
    "            'DecisionTreeRegressor': (DecisionTreeRegressor(), {'max_depth': [3, 5]}),\n",
    "        }\n",
    "\n",
    "        \n",
    "    def evaluate_models(self):\n",
    "        \"\"\"Evaluates all combinations of trend and seasonality models using GridSearchCV.\"\"\"\n",
    "        self.define_models()\n",
    "        tscv = TimeSeriesSplit(n_splits=5)  # Define the TimeSeriesSplit\n",
    "        best_params = {}\n",
    "        best_scores = {}\n",
    "\n",
    "        for trend_model in self.trend_models:\n",
    "            for model_name, (seasonality_model, param_grid) in self.seasonality_models.items():\n",
    "                model = BoostedHybrid(model_1=trend_model, model_2=seasonality_model)\n",
    "                \n",
    "                # Use GridSearchCV to find the best hyperparameters\n",
    "                grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=tscv)\n",
    "                grid_search.fit(X_train, y_train)\n",
    "\n",
    "                # Get the best parameters and score\n",
    "                best_params[model_name] = grid_search.best_params_\n",
    "                best_scores[model_name] = -grid_search.best_score_  # Negate to get the positive MSE\n",
    "\n",
    "                # Evaluate the best model on the validation set\n",
    "                best_model = grid_search.best_estimator_\n",
    "                y_val_pred = best_model.predict(X_val)\n",
    "                val_mse = mean_squared_error(y_val, y_val_pred)\n",
    "                # Update the best model if the current one is better\n",
    "                if val_mse < self.best_score:\n",
    "                    self.best_score = val_mse\n",
    "                    self.best_model = best_model\n",
    "\n",
    "    def save_best_model(self, model_path):\n",
    "        \"\"\"Saves the best found model.\"\"\"\n",
    "        if self.best_model:\n",
    "            self.best_model.save_model(model_path)\n",
    "        else:\n",
    "            print(\"No model has been evaluated yet.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aff937-f464-42c6-b9a2-dbbe3bf78c7e",
   "metadata": {},
   "source": [
    "# Define Hybrid_Model_Loader which is used for save and load the hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4aec7-6a41-46b2-a5e1-e72c512146ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_Model_Loader:\n",
    "    @staticmethod\n",
    "    def save_model(boosted_hybrid, file_path):\n",
    "        \"\"\"\n",
    "        save_model: Save the state of the BoostedHybrid model to a specified file.\n",
    "\n",
    "        Parameters:\n",
    "            boosted_hybrid (BoostedHybrid): The BoostedHybrid instance to save.\n",
    "            file_path (str): The path where the model state will be saved.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'wb') as f:\n",
    "            # Save models and their state\n",
    "            pickle.dump({\n",
    "                'model_1': boosted_hybrid.model_1,\n",
    "                'model_2': boosted_hybrid.model_2,\n",
    "                'poly': boosted_hybrid.poly,\n",
    "                'poly_model': boosted_hybrid.poly_model,\n",
    "            }, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(file_path):\n",
    "        \"\"\"\n",
    "        load_model :Load the BoostedHybrid model state from a specified file.\n",
    "\n",
    "        Parameters:\n",
    "             inputs: file_path (str): The path from which the model state will be loaded.\n",
    "\n",
    "        Returns:\n",
    "            BoostedHybrid: An instance of the BoostedHybrid model with the loaded state.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            model_1 = data['model_1']\n",
    "            model_2 = data['model_2']\n",
    "            poly = data['poly']\n",
    "            poly_model = data['poly_model']\n",
    "            boosted_hybrid = BoostedHybrid(model_1, model_2)\n",
    "            boosted_hybrid.poly = poly\n",
    "            boosted_hybrid.poly_model = poly_model\n",
    "            return boosted_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb562f3-48c9-49b0-958d-a9325415e004",
   "metadata": {},
   "source": [
    "# Load the datasets and doing the prepocessing part\r\n",
    "## split the data into traitn parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6bb35-70dc-4c6f-9330-2664ccdf38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"Train\"\n",
    "train_datasets = DataLoader.load_data(train_folder)\n",
    "\n",
    "# Apply the time extraction function to each dataset in train_datasets\n",
    "for key in train_datasets:\n",
    "    data_processor=Dataset_Processing(train_datasets[key],key,'value')\n",
    "    data_processor.data_processing()\n",
    "    train_datasets[key]=data_processor.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4384c-7e28-487a-80e8-9a0bc11c5ae0",
   "metadata": {},
   "source": [
    "#  example to use\r\n",
    "## Get the best model for data 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb3028-8c64-4c3f-b5c8-3debd8ab462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_key=9\n",
    "train, val = time_series_train_val_split(train_datasets[data_key], train_size=0.8)\n",
    "\n",
    "X_train=train.drop(columns='value')\n",
    "y_train=train['value']\n",
    "\n",
    "X_val=train.drop(columns='value')\n",
    "y_val=train['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733b3f9-0ed9-40fb-8505-3c3fb890806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class with training datasets\n",
    "model_getter = Model_Getter(train_datasets[data_key])\n",
    "\n",
    "# Evaluate all models to find the best one\n",
    "model_getter.evaluate_models()\n",
    "\n",
    "# Save the best model\n",
    "model_getter.save_best_model('boosted_hybrid_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03207e-90fc-42d9-a609-a806f9446678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "loaded_model = Hybrid_Model_Loader.load_model('boosted_hybrid_model.pkl')\n",
    "\n",
    "# Making predictions\n",
    "true_y = train_datasets[data_key]['value']\n",
    "pred_y = loaded_model.predict(train_datasets[data_key].drop('value', axis=1))\n",
    "TimeSeriesPlotter.plot_actual_vs_predicted( true_y[0:200], pred_y[0:200])\n",
    "mae = mean_absolute_error(true_y, pred_y)\n",
    "mse = mean_squared_error(true_y, pred_y)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(true_y, pred_y)\n",
    "\n",
    "print(f'MAE: {mae}, MSE: {mse}, RMSE: {rmse}, R: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d504d62-b0c8-4b7c-b9d0-1f85b50d98a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
